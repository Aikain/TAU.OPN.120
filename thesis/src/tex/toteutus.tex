\chapter{Kielimallit todellisessa käytössä}%
\label{ch:toteutus}

Tähän tulee tietoa toteutuksesta.

\section{Vertex AI: Gemini \& PaLM 2}

Vertex AI -alustan käyttöön on tarjolla kirjastoja useille eri
ohjelmointikielille \parencite{vertexAiGenerativeAiQuickstart}. Sovelluksen
backend on toteutettu javalla ja pakettimanagerina on maven, joten Vertex AI:n
javalle tekemän kirjasto saadaan käyttöön lisäämällä pom.xml-tiedostoon
esimerkin \ref{lst:vertexai-pom.xml} mukaisesti.

\begin{lstlisting}[
    basicstyle=\small,
    caption={google-cloud-vertexai riippuvuus pom.xml tiedostoon},
    label={lst:vertexai-pom.xml},
    language=xml,
]
<dependency>
    <groupId>com.google.cloud</groupId>
    <artifactId>google-cloud-vertexai</artifactId>
    <version>1.2.0</version>
</dependency>
\end{lstlisting}

Kevään 2024 aikana riippuvuuteen on tullut useita versioita, joista osa on
tuonut rikkovia muutoksia \parencite{mavenGoogleVertexAIAPI}. Kirjastoon
liittyvät esimerkit ovat tehty versiolle 1.2.0.

Sovellukseen on toteutettu backendin puolelle interface, jonka toteuttamalla
serviceä pystytään käyttämään helposti UI:n kanssa. Toteuttamalla interfacen
saadaan luotua tarina halutulla mallilla sekä jatkettua tarinaa. Liitteessä
\ref{ch:gemini-service} on kyseisille metodeille tehdyt toteutukset, joissa
luodaan VertexAI:n kirjastosta löytyvät ChatSession ja laitetaan se talteen
niin tarinaa voidaan jatkaa myöhemmin.

PaLM 2:lle saadaan puolestaan toteutettua liitteen \ref{ch:palm2-service}
mukaisesti. PaLM 2:lle ei ole tarjolla vastaavaa ChatSessionia, kuin Geminille,
joten on sille toteutettu vain tarinan luonti, muttei mahdollisuutta jatkaa
tarinaa.

\section{Llama 3.1}

Vertex AI:n Model Gardenissa on ollut 23.07.2024 alkaen tarjolla Llama 3.1:sta
API Service, joka mahdollistaa Llama 3.1:n käyttämisen vastaavasta kuin
Geminin. Tämän ansiosta mallia ei tarvitse itse pystyttää, jolle laskutus olisi
tuntipohjainen vaan laskutus tapahtuu käytettyjen tokenien mukaan.
\parencite{vertexAiModelGardenLlama3}

Vertex AI:n SDK tukee valmiiksi useita eri modeleita, myös muita kuin Geminin
eri variaatioita, joten toteutus voi olla käytännössä täysin sama kuin aiemmin
esiltelty toteutus Geminille liitteessä \ref{ch:gemini-service}. Luomalla oman
servicen saadaan kuitenkin mukautettua sisällössä annettava ohje sekä
parametrit sopivammaksi Llama 3.1-mallille.

\section{Claude 3.5 Sonnet}

Anthropic tarjoaa vain python kirjastoja Clauden käyttöön, joten sovellukseen
integrointi tehdään kutsuen suoraan saatavilla olevaa rajapintaa. Yksi
yksinkertainen vaihtoehto on käyttää openfeign:iä, joka saadaan käyttöön
lisäämällä `pom.xml`:n riippuvuuksiin \ref{lst:openfeign-pom.xml} mukainen
riippuvuus.

\begin{lstlisting}[
    basicstyle=\small,
    caption={spring-cloud-starter-openfeign riippuvuus pom.xml tiedostoon},
    label={lst:openfeign-pom.xml},
    language=xml,
]
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
    <version>4.1.3</version>
</dependency>
\end{lstlisting}

Openfeign riippuvuuden lisäämisen jälkeen saadaan rajapinta käyttöön liitteen
\ref{ch:claude-example} esimerkkien mukaisesti. Esimerkissä luodaan ensin
interface, jossa on tarvittavat päätepisteet. Rajapinnan käyttäminen vaatii,
että versio ja API avain on määritelty otsikko -tietueisiin
\parencite{anthropicAPIDocsVersions}
\parencite{anthropicAPIDocsGettingStarted}. Tämä on toteutettu esimerkissä
näkyvällä ClaudeConfig-luokan toteutuksella, jossa annetaan
RequestInterceptor:lle haluttu API avain ympristömuuttujasta. Interfacen
määrityksen sekä vaadittujen otsikkotietojen asettamisen jälkeen voidaan API:a
kutsua esimerkissä olevan ClaudeService-luokan toteutuksen mukaisesti.
