@online{akavaworksNakokulmiaTekoalyynOsa6,
    title = {Näkökulmia tekoälyyn, osa 6 – Nikolaj Tatti: Tekoälyn rajoitukset ja väärinkäyttö},
    date = {2024-04-05},
    url = {https://akavaworks.fi/julkaisut/artikkelit/nakokulmia-tekoalyyn-osa-6-nikolaj-tatti-tekoalyn-rajoitukset-ja-vaarinkaytto/},
    urldate = {2025-01-12},
    langid = {finnish},
}

@online{alberttechStrawberry,
    title = {Why no AI can solve this question},
    date = {2024-08-26},
    url = {https://www.youtube.com/shorts/7pQrMAekdn4},
    urldate = {2025-01-12},
    langid = {english},
}

@book{alma9911410816105973,
    author = {Salo, Immo and Helsingin seudun kauppakamari, kustantaja.},
    address = {Helsinki},
    booktitle = {Luova tekoäly mullistaa kaiken : ChatGPT näyttää tietä},
    edition = {1. painos.},
    isbn = {978-952-246-900-7},
    keywords = {tekoäly},
    language = {fin},
    publisher = {Kauppakamari},
    title = {Luova tekoäly mullistaa kaiken : ChatGPT näyttää tietä },
    year = {2023},
}

@book{alma9911564814005973,
    author = {Salo, Immo and Helsingin seudun kauppakamari, kustantaja.},
    address = {Helsinki},
    booktitle = {Luova tekoäly työn supervoimana},
    edition = {1. painos.},
    isbn = {978-952-246-957-1},
    keywords = {tekoäly},
    language = {fin},
    publisher = {Kauppakamari},
    title = {Luova tekoäly työn supervoimana },
    year = {2024},
}

@online{anthropicAPIDocs,
    title = {Welcome to Cloude - Anthropic},
    date = {2024-08-05},
    url = {https://docs.anthropic.com/en/docs/welcome},
    urldate = {2024-08-05},
    langid = {english},
}

@online{anthropicAPIDocsGettingStarted,
    title = {Getting started - Anthropic},
    date = {2024-08-05},
    url = {https://docs.anthropic.com/en/api/getting-started},
    urldate = {2024-08-05},
    langid = {english},
}

@online{anthropicAPIDocsModels,
    title = {Models - Anthropic},
    date = {2024-08-05},
    url = {https://docs.anthropic.com/en/docs/about-claude/models},
    urldate = {2024-08-05},
    langid = {english},
}

@online{anthropicAPIDocsVersions,
    title = {Versions - Anthropic},
    date = {2024-08-05},
    url = {https://docs.anthropic.com/en/api/versioning},
    urldate = {2024-08-05},
    langid = {english},
}

@online{anthropicClaude,
    title = {Meet Claude \ Anthropic},
    date = {2024-07-18},
    url = {https://www.anthropic.com/claude},
    urldate = {2024-08-05},
    langid = {english},
}

@online{anthropicClaudeSonnet,
    title = {Claude 3.5 Sonnet \ Anthropic},
    date = {2024-10-22},
    url = {https://www.anthropic.com/claude/sonnet},
    urldate = {2024-10-30},
    langid = {english},
}

@online{anthropicClaudeHaiku,
    title = {Claude 3.5 Haiku \ Anthropic},
    date = {2024-10-22},
    url = {https://www.anthropic.com/claude/haiku},
    urldate = {2024-10-30},
    langid = {english},
}

@online{anthropicClaudeSonnetAndHaiku35,
    title = {Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku \ Anthropic},
    date = {2024-10-22},
    url = {https://www.anthropic.com/claude/haiku},
    urldate = {2024-10-30},
    langid = {english},
}

@online{anthropicCompany,
    title = {Company \ Anthropic},
    date = {2024-06-20},
    url = {https://www.anthropic.com/company},
    urldate = {2024-08-05},
    langid = {english},
}

@online{anthropicPricing,
    title = {Pricing \ Anthropic},
    date = {2024-06-25},
    url = {https://www.anthropic.com/pricing},
    urldate = {2024-08-05},
    langid = {english},
}

@online{anthropicResearch,
    title = {Research \ Anthropic},
    date = {2024-06-20},
    url = {https://www.anthropic.com/research},
    urldate = {2024-08-05},
    langid = {english},
}

@online{baeldungSpringBean,
    title = {What Is a Spring Bean? | Baeldung},
    date = {2024-06-10},
    url = {https://www.baeldung.com/spring-bean},
    urldate = {2024-12-15},
    langid = {english},
}

@article{benchmarkGPQA,
    author = {Rein, David and Betty Li Hou and Asa Cooper Stickland and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Julian, Michael and Bowman, Samuel R},
    address = {Ithaca},
    copyright = {2023. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn = {2331-8422},
    journal = {arXiv.org},
    keywords = {Supervisors},
    language = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title = {GPQA: A Graduate-Level Google-Proof Q\&A Benchmark},
    year = {2023},
}

@article{benchmarkHumanEval,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year={2021},
  eprint={2107.03374},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{benchmarkMATH,
    address = {Ithaca},
    copyright = {2021. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn = {2331-8422},
    journal = {arXiv.org},
    keywords = {Machine learning ; Problem solving},
    language = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title = {Measuring Mathematical Problem Solving With the MATH Dataset},
    year = {2021},
}

@INPROCEEDINGS{benchmarkMMMU,
    author={Yue, Xiang and Ni, Yuansheng and Zheng, Tianyu and Zhang, Kai and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
    booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    title={MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
    year={2024},
    volume={},
    number={},
    pages={9556-9567},
    keywords={Computer vision;Computational modeling;Artificial general intelligence;Social sciences;Manuals;Benchmark testing;Cognition;Large Multimodal Models;;Evaluation;Multimodal Large Language Models;LMMs;Large Language Models;LLMs},
    doi={10.1109/CVPR52733.2024.00913},
}

@article{benchmarkMSGM,
    abstract = {We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.},
    author = {Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and Das, Dipanjan and Wei, Jason},
    address = {Ithaca},
    copyright = {2022. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn = {2331-8422},
    journal = {arXiv.org},
    keywords = {Bench-marks ; Languages ; Natural Language Processing ; Reasoning ; Translating},
    language = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title = {Language Models are Multilingual Chain-of-Thought Reasoners},
    year = {2022},
}

@article{benchmarkDROP,
    abstract = {Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.},
    author = {Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
    address = {Ithaca},
    copyright = {2019. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn = {2331-8422},
    journal = {arXiv.org},
    keywords = {Bench-marks ; Mathematical models ; Reading comprehension ; Reasoning},
    language = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title = {DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
    year = {2019},
}

@online{claudeChat,
    title = {Claude},
    date = {2024-08-05},
    url = {https://claude.ai/new},
    urldate = {2024-08-05},
    langid = {english},
}

@online{claudePricing,
    title = {Claude - Choose your plan},
    date = {2024-08-05},
    url = {https://claude.ai/upgrade},
    urldate = {2024-08-05},
    langid = {english},
}

@misc{copet2024simplecontrollablemusicgeneration,
    title={Simple and Controllable Music Generation},
    author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre Défossez},
    year={2024},
    eprint={2306.05284},
    archivePrefix={arXiv},
    primaryClass={cs.SD},
    url={https://arxiv.org/abs/2306.05284},
}

@online{evlSuuriIhmeRippikoulusuunnitelma2017,
    title = {Suuri Ihme – Rippikoulusuunnitelma 2017},
    date = {2017},
    url = {https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf},
    urldate = {2024-12-15},
    langid = {finnish},
}

@online{fireshipElonsGrokAI,
    title = {Elon’s "based" Grok AI has entered the chat…},
    date = {2024-12-10},
    url = {https://www.youtube.com/watch?v=CgruI1RjH_c},
    urldate = {2024-12-16},
    langid = {english},
}

@online{fireshipElonsLawsuitAgainstOpenAI,
    title = {Elon's bombshell lawsuit against OpenAI},
    date = {2024-03-02},
    url = {https://www.youtube.com/watch?v=KbzGy3whpy0},
    urldate = {2024-12-16},
    langid = {english},
}

@online{geminiUpdates,
    title = {Gemini Appsin julkaisupäivitykset ja parannukset},
    date = {2024-06-11},
    url = {https://gemini.google.com/updates?hl=fi},
    urldate = {2024-06-11},
    langid = {finnish},
}

@online{gemini2,
    title = {Gemini 2.0 is now available to everyone},
    date = {2025-02-05},
    url = {https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/},
    urldate = {2024-02-07},
    langid = {english},
}

@online{githubBerriAIlitellmIssues6610,
    title = {[Bug]: Does x-ai not support the response\_format parameter? · Issue \#6610 · BerriAI/litellm},
    date = {2024-11-06},
    url = {https://github.com/BerriAI/litellm/issues/6610},
    urldate = {2024-12-16},
    langid = {english},
}

@online{githubSunoAiBark,
    title = {suno-ai/bark: Text-Prompted Generative Audio Model},
    date = {2024-05-04},
    url = {https://github.com/suno-ai/bark},
    urldate = {2025-01-12},
    langid = {english},
}

@online{googleAiAvailableRegions,
    title = {Available regions for Google AI Studio and Gemini API | Google AI for Developers | Google for Developers},
    date = {2024-05-21},
    url = {https://ai.google.dev/gemini-api/docs/available-regions},
    urldate = {2024-06-11},
    langid = {english},
}

@online{googleDeepmindGemini,
    title = {Gemini - Google Deepmind},
    date = {2024-06-11},
    url = {https://deepmind.google/technologies/gemini/},
    urldate = {2024-06-11},
    langid = {english},
}

@online{googleDeepmindGeminiv1report,
    title = {Gemini: A Family of Highly Capable Multimodal Models},
    date = {2023-12},
    url = {https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf},
    urldate = {2024-06-11},
    langid = {english},
}

@online{googleDeepmindGeminiv1_5report,
    title = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
    date = {2024-05},
    url = {https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf},
    urldate = {2024-06-11},
    langid = {english},
}

@online{googleDeepmindImagen3_v3report,
    title = {Imagen 3},
    date = {2024-12-21},
    url = {https://storage.googleapis.com/deepmind-media/imagen/imagen_3_tech_report_update_dec2024_v3.pdf},
    urldate = {2025-01-12},
    langid = {english},
}

@online{googleKeynote2023,
    title = {Google Keynote (Google I/O ‘23)},
    date = {2023-05-10},
    url = {https://www.youtube.com/watch?v=cNfINi5CNbY},
    urldate = {2024-06-11},
    langid = {english},
}

@online{googleKeynote2024,
    title = {Google Keynote (Google I/O ‘24)},
    date = {2024-05-14},
    url = {https://www.youtube.com/watch?v=XEzRZ35urlk},
    urldate = {2024-06-11},
    langid = {english},
}

@online{googlePaLMAPIDeprecated,
    title = {PaLM API deprecation | Google AI for Developers | Google for Developers},
    date = {2024-04-11},
    url = {https://ai.google.dev/palm_docs/deprecation},
    urldate = {2024-06-16},
    langid = {english},
}

@online{googlePaLM2Introducing,
    title = {Google AI: What to know about the PaLM 2 large language model},
    date = {2023-05-10},
    url = {https://blog.google/technology/ai/google-palm-2-ai-large-language-model/},
    urldate = {2024-06-16},
    langid = {english},
}

@online{googlePaLM2TechReport,
    title = {PaLM 2 Technical Report},
    date = {2023-05},
    url = {https://ai.google/static/documents/palm2techreport.pdf},
    urldate = {2024-06-16},
    langid = {english},
}

@online{haukkaJimiKuinkakielimallitOppivat,
    title = {Kuinka suuret kielimallit oppivat ymmärtämään ja tuottamaan kieltä?},
    date = {2024-02-06},
    url = {http://urn.fi/URN:NBN:fi:jyu-202402061757},
    urldate = {2025-01-12},
    langid = {finnish},
}

@misc{kaplan2020scalinglawsneurallanguage,
    title={Scaling Laws for Neural Language Models},
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    eprint={2001.08361},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2001.08361},
}

@misc{liu2023audioldmtexttoaudiogenerationlatent,
    title={AudioLDM: Text-to-Audio Generation with Latent Diffusion Models},
    author={Haohe Liu and Zehua Chen and Yi Yuan and Xinhao Mei and Xubo Liu and Danilo Mandic and Wenwu Wang and Mark D. Plumbley},
    year={2023},
    eprint={2301.12503},
    archivePrefix={arXiv},
    primaryClass={cs.SD},
    url={https://arxiv.org/abs/2301.12503},
}

@online{llama,
    title = {Llama 3.1},
    date = {2024-08-06},
    url = {https://llama.meta.com/},
    urldate = {2024-08-06},
    langid = {english},
}

@article{llama1,
    author = {{Touvron}, Hugo and {Lavril}, Thibaut and {Izacard}, Gautier and {Martinet}, Xavier and {Lachaux}, Marie-Anne and {Lacroix}, Timoth{\'e}e and {Rozi{\`e}re}, Baptiste and {Goyal}, Naman and {Hambro}, Eric and {Azhar}, Faisal and {Rodriguez}, Aurelien and {Joulin}, Armand and {Grave}, Edouard and {Lample}, Guillaume},
    title = {Llama: Open and Efficient Foundation Language Models},
    journal = {arXiv e-prints},
    keywords = {Computer Science - Computation and Language},
    year = 2023,
    month = feb,
    eid = {arXiv:2302.13971},
    pages = {arXiv:2302.13971},
    doi = {10.48550/arXiv.2302.13971},
    archivePrefix = {arXiv},
    eprint = {2302.13971},
    primaryClass = {cs.CL},
}

@online{llama2,
    title = {Meta and Microsoft Introduce the Next Generation of Llama},
    date = {2023-07-18},
    url = {https://ai.meta.com/blog/llama-2/},
    urldate = {2024-08-06},
    langid = {english},
}

@online{llama3,
    title = {Introducing Meta Llama 3: The most capable openly available LLM to date},
    date = {2024-04-18},
    url = {https://ai.meta.com/blog/meta-llama-3/},
    urldate = {2024-08-06},
    langid = {english},
}

@online{llama31,
    title = {Introducing Llama 3.1: Our most capable models to date},
    date = {2024-07-23},
    url = {https://ai.meta.com/blog/meta-llama-3-1/},
    urldate = {2024-08-06},
    langid = {english},
}

@online{llamaGuard3,
    title = {Llama 3 Guard 3 | Model Cards \& Prompt formats},
    date = {2024-08-06},
    url = {https://llama.meta.com/docs/model-cards-and-prompt-formats/llama-guard-3},
    urldate = {2024-08-06},
    langid = {english},
}

@online{llamaOtherModels,
    title = {Other Models | Model Cards \& Prompt formats},
    date = {2024-08-06},
    url = {https://llama.meta.com/docs/model-cards-and-prompt-formats/other-models},
    urldate = {2024-08-06},
    langid = {english},
}

@online{mavenGoogleVertexAIAPI,
    title = {Maven Repository: com.google.cloud » google-cloud-vertexai},
    date = {2024-06-08},
    url = {https://mvnrepository.com/artifact/com.google.cloud/google-cloud-vertexai},
    urldate = {2024-06-17},
    langid = {english},
}

@misc{openAI2023GPT4,
    title={GPT-4 Technical Report},
    author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
    year={2023},
    eprint={2303.08774},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@online{openAIDallE,
    title = {DALL·E: Creating images from text | OpenAI},
    date = {2021-01-05},
    url = {https://openai.com/index/dall-e/},
    urldate = {2025-01-12},
    langid = {english},
}

@online{openAIGPT4o,
    title = {Hello GPT-4o | OpenAI},
    date = {2024-05-13},
    url = {https://openai.com/index/hello-gpt-4o/},
    urldate = {2024-10-30},
    langid = {english},
}

@online{openAISoraReport,
    title = {Video generation models as world simulators | OpenAI},
    date = {2024-02-15},
    url = {https://openai.com/index/video-generation-models-as-world-simulators/},
    urldate = {2024-10-08},
    langid = {english},
}

@online{stableDiffusionLaunch,
    title = {Stable Diffusion Launch Announcement - Stability Ai},
    date = {2022-08-10},
    url = {https://stability.ai/news/stable-diffusion-announcement},
    urldate = {2025-01-12},
    langid = {english},
}

@online{twitter1547108864788553729,
    title = {We're officially moving to open-beta!},
    date = {2022-07-13},
    author = {@midjourney},
    howpublished = {X.com},
    url = {https://x.com/midjourney/status/1547108864788553729},
    urldate = {2025-01-12},
}

@online{twitter1679182035645235200,
    title = {And what are the most fundamental unknown questions?},
    date = {2023-07-12},
    author = {@elonmusk},
    howpublished = {X.com},
    url = {https://x.com/elonmusk/status/1679182035645235200},
    urldate = {2024-12-16},
}

@online{twitter1758192957386342435,
    title = {Introducing Sora, our text-to-video model},
    date = {2024-02-15},
    author = {@OpenAI},
    howpublished = {X.com},
    url = {https://x.com/OpenAI/status/1758192957386342435},
    urldate = {2024-10-08},
}

@online{udioAboutUs,
    title = {About Udio - AI-Powered Music Creation for All},
    url = {https://www.udio.com/about-us},
    urldate = {2025-01-12},
    langid = {finnish},
}

@online{valtioneuvostoSuomenTekoalyaika,
    title = {Suomen tekoälyaika – Suomi tekoälyn soveltamisen kärkimaaksi: Tavoite ja toimenpidesuositukset},
    date = {2017-10-23},
    url = {http://urn.fi/URN:ISBN:978-952-327-248-4},
    urldate = {2024-10-07},
    langid = {finnish},
}

@online{vertexAiGenerativeAiQuickstart,
    title = {Quickstart: Send requests to the Vertex AI API for Gemini | Generative AI on Vertex AI | Google Cloud},
    date = {2024-06-14},
    url = {https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal#set-up-your-environment},
    urldate = {2024-06-17},
    langid = {english},
}

@online{vertexAiGenerativeAiPricing,
    title = {Pricing | Generative AI on Vertex AI | Google Cloud},
    date = {2024-05-28},
    url = {https://cloud.google.com/vertex-ai/generative-ai/pricing},
    urldate = {2024-06-11},
    langid = {english},
}

@online{vertexAiModelGardenLlama3,
    title = {Llama 3.1 API Service - Vertex AI - Google Cloud console},
    date = {2024-07-23},
    url = {https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3-405b-instruct-maas},
    urldate = {2024-08-05},
    langid = {english},
}

@online{xAIAbout,
    title = {About xAI},
    date = {2024-12},
    url = {https://x.ai/about},
    urldate = {2024-12-16},
    langid = {english},
}

@online{xAIBlogApi,
    title = {API Public Beta},
    date = {2024-11-04},
    url = {https://x.ai/blog/api},
    urldate = {2024-12-16},
    langid = {english},
}

@online{xAIDocsEndpoints,
    title = {Welcome to the xAI documentation},
    date = {2024-10},
    url = {https://docs.x.ai/api/endpoints},
    urldate = {2024-12-16},
    langid = {english},
}

@online{xAIDocsIntegrations,
    title = {Welcome to the xAI documentation},
    date = {2024-10},
    url = {https://docs.x.ai/api/integrations},
    urldate = {2024-12-16},
    langid = {english},
}
